---
layout: post
title:  "支持向量机"
categories: 机器学习
tags: svm 支持向量机
author: Benjamin
---

* content
{:toc}

机器学习常用算法梳理记录。




<h1 id="1">SVM</h1>

<h2 id="1.1"> 从逻辑回归到SVM</h2>

<h3 id="1.1.1"> 重新审视logistic回归</h3>

逻辑回归的目的是从数据集中学习一个0/1分类模型，
这个模型将特征的线性组合作为自变量，自变量的取值
范围是负无穷到正无穷。通过逻辑函数将自变量映射到
（0， 1）上，映射后的值被认为是y=1的概率。

公式如下：

$$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+exp(-\theta^Tx)}$$

其中$x$是$n$维特征向量，函数$g$就是logistic函数$g(z)=\frac{1}{1 + e^{-z}}$, $g(z)$ 的图像如下：

![1](1.png)

可以看到，将无穷映射到了（0 ，1）。
而假设函数 $h_\theta(x)$ 就是数据属于$y=1$的概率。

$P(y=1 | x;\theta)=h_\theta(x)$

$P(y=0 | x;\theta)=1-h_\theta(x)$

当我们要判别一个新来的数据属于哪个类时，只需求$h_\theta(x)$，若大于0.5就是$y=1$的类，反之就属于$y=0$的类。
再审视下$h_\theta(x)$，发现$h_\theta(x)$ 只和 $\theta^Tx$ 有关，$\theta^Tx>0$，那么 $h_\theta(x) > 0.5$，$g(z)$ 只不过是用来映射，真实的类别决定权还在 $\theta^Tx$。还有当 $\theta^Tx >> 0$ 时，$h_\theta(x) = 1$，反之 $h_\theta(x) = 0$。如果我们只从 $\theta^Tx$ 出发，希望模型达到的目标无非就是让训练数据中 $y=1$ 时，$\theta^Tx >> 0$，而 $y=0$ 时，$\theta^Tx << 0$，强调在全部训练实例上达到这个目标。

图形化表示如下：

![2](2.png)

中间那条线是 $\theta^Tx = 0$，logistic强调所有数据点尽可能远离中间那条线。以上面3个点A、B、C为例，从图中我们可以确定A是X类别的，然而C我们是不太确定的，因为它离分割线比较近。我们是不是应该更关心那些像C这种靠近分割线的点，让它们尽可能远离中间线，而不是所有点上达到最优。因为那样的话，会使得部分点靠近中间线来换取另外部分点更加远离中间线。

这里就要引出SVM的思想了，SVM不关心已经确定远离的点，只关心那些离分割超平面最近的那些点——支持向量点。SVM目的是为了学到一个分离超平面，使得数据集中距离分离超平面最近的点距离分离超平面最远。

<h2 id="1.2"> 线性可分线性支持向量机</h2>

定义：给定线性可分的训练数据集，通过间隔最大化学习到的分离超平面为：
$w^Tx + b = 0$

以及相应的分类决策函数：
$f(x) = sign(w^Tx + b)$。称为线性可分支持向量机。

从定义我们也看出了支持向量机的类别标签不是0和1，变成+1和-1了。

那么如何利用间隔最大化来学习到分离超平面呢？这里先不讲，我们再来回顾下前面的图。

直观来看，点A距分离超平面较远，若预测该点为正类，就比较确信预测是正确的；点C距离分离超平面较近，若预测该点为正类就不那么确信了；点B介于点A和点C之间，预测它为正类的确信度也在A和C之间。

所以一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面 $w^Tx + b = 0$ 确定的情况下，$|w^Tx+b|$ 能相对地表示点 $x$ 距离超平面的远近。而$w^Tx + b$ 的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可用量 $y(w^Tx + b)$ 来表示分类的正确性及确信度，这就是函数间隔的概念。

<h3 id="1.2.1">函数间隔</h3>

对于给定的训练数据集 $T$ 和超平面 $(w, b)$ ，定义超平面 $(w, b)$ 关于样本点 $(x_i, y_i)$ 的函数间隔为：

$$\hat{\gamma_{i}} = y_i(w^Tx_i+b)$$

定义超平面 $(w, b)$ 关于训练数据集 $T$的函数间隔为超平面 $(w,b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的函数间隔的最小值，即

$$\hat{\gamma} = \min_{i=1,\dots,N}\hat{\gamma_{i}}$$

函数间隔可以表示分类预测的正确性及确信度。但是在选择分离超平面时，只有函数间隔还不够，因为只要成比例地改变$w$和$b$，超平面并没有改变，但是函数间隔变化了。为了使得函数间隔是确定的，可以对分离超平面的法向量进行约束，如规范化，让 $||w||=1$，此时函数间隔就变成了几何间隔。

<h3 id="1.2.2"> 几何间隔</h3>

对于给定的训练数据集 $T$ 和超平面 $(w, b)$，定义超平面 $(w, b)$  关于样本点 $(x_i, y_i)$ 的几何间隔为：

$$\gamma_{i} = \frac{y_i(w^Tx_i+b)}{||w||}$$

定义超平面 $(w, b)$ 关于训练数据集 $T$的函数间隔为超平面 $(w,b)$ 关于 $T$ 中所有样本点 $(x_i, y_i)$ 的几何间隔的最小值，即

$$\gamma = \min_{i=1,\dots,N}\gamma_{i}$$

这里几何间隔的概念是通过函数间隔的变化得来的，也可以直接通过计算推导得出这个公式。

如下图所示：

![3](3.png)

假设样本点A $(x_i, 1)$ 到B点$x$的距离为 $\gamma_i$，则 $\vec{BA}=\vec{A}-\vec{B}=\frac{\gamma_i\vec{w}}{||w||}$，则
$$x=x_i-\frac{\gamma_i\vec{w}}{||w||}$$
又因为 $w^Tx+b=0$，则
$$w^T(x_i-\frac{\gamma_i\vec{w}}{||w||})+b=0$$
$$\gamma_i=\frac{w^Tx_i+b}{||w||}$$
同理，当A点在超平面下方时
$$\gamma_i=\frac{-(w^Tx_i+b)}{||w||}$$
也就是几何间隔的定义：
$$\gamma_{i} = \frac{y_i(w^Tx_i+b)}{||w||}$$

<h3 id="1.2.3"> 间隔最大化</h3>

已经知道了几何间隔的定义，下面该考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，这个问题可以表示为下面的约束最优化问题：

$$
\begin{aligned}
&\max_{w,b}\quad\gamma \\
&s.t.\quad \frac{y_i(w^Tx_i+b)}{||w||}\geq\gamma，i=1,2,\dots,N
\end{aligned}
$$

考虑到几何间隔和函数间隔的关系，可以将上式改写为：
$$
\begin{aligned}
&\max_{w,b}\quad\frac{\hat{\gamma}}{||w||} \\
&s.t.\quad y_i(w^Tx_i+b)\geq\hat{\gamma}，i=1,2,\dots,N
\end{aligned}
$$

而函数间隔的取值并不影响上述最优化问题的解。事实上，假设将$w$和$b$按比例改变为 $\lambda w$和 $\lambda b$，这时函数间隔成为 $\lambda \hat{\gamma}$。函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一个等价的最优化问题。这样，我们直接可以取 $\hat{\gamma}=1$。注意到最大化 $\frac{1}{||w||}$ 与最小化 $\frac{1}{2}||w||^2$ 是等价的，于是就得到下面的线性可分支持向量机学习的最优化问题：
$$
\begin{aligned}
&\min_{w,b}\quad\frac{1}{2}||w||^2 \\
&s.t.\quad y_i(w^Tx_i+b)-1\geq0，i=1,2,\dots,N
\end{aligned}
$$

这个最优化问题可以通过另一种方式来推导。。。

怎么求解这个凸二次规划问题呢？

![](约束条件.jpg)

![](最优解.jpg)
<h3 id="1.2.4"> 对偶算法</h3>

首先构建拉格朗日函数。对每一个不等式约束引进拉格朗日乘子 $\alpha_i\geq0，i=1,2,\dots,N$,定义拉格朗日函数：
$$L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^N\alpha_iy_i(w^Tx_i+b) + \sum_{i=1}^N\alpha_i$$

$$
\max_{\alpha}L(w,b,\alpha) =
\begin{cases}
  \frac{1}{2}||w||^2, & y_i(w^Tx_i+b)-1\geq0，i=1,2,\dots,N \\
  \infty, & 其它
\end{cases}
$$

设：
$$\theta_P(w,b)=\max_{\alpha}L(w,b,\alpha)$$

$$\theta_D(\alpha)=\min_{w,b}L(w,b,\alpha)$$
则原始问题可以变为
 $$\min_{w,b}\theta_P(w,b)$$
 它的对偶问题为：
 $$\max_{\alpha}\theta_D(\alpha)$$

$$
\begin{aligned}
&\theta_D(\alpha)\leq L(w,b,\alpha)\leq \theta_P(w,b) \\
& \max_{\alpha}\theta_D(\alpha)\leq \min_{w,b}\theta_P(w,b)
\end{aligned}
$$

当满足下列条件时：
$$
\begin{aligned}
&\triangledown_wL(w,b,\alpha)=0\\ &\triangledown_bL(w,b,\alpha)=0\\
&\alpha_i(y_i(w^Tx_i+b)-1)=0,\quad i=1,2,\dots,N\\
&y_i(w^Tx_i+b)-1\geq0,\quad i=1,2,\dots,N\\
&\alpha_i\geq0,\quad i=1,2,\dots,N
\end{aligned}
$$

[详细证明](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)

$$
\begin{aligned}
&w=\sum_{i=1}^N\alpha_iy_ix_i\\
&\sum_{i=1}^N\alpha_iy_i=0
\end{aligned}
$$

将$w$的表达式代入拉格朗日函数，则：
$$\min_{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i^Tx_j)+\sum_{i=1}^N\alpha_i$$

则 $\max_{\alpha}\theta_D(\alpha)$ 转化为下面的约束优化问题：

 $$
\begin{aligned}
\max &-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j(x_i^Tx_j)+\sum_{i=1}^N\alpha_i\\ s.t.&\sum_{i=1}^N\alpha_iy_i=0\\
&\alpha_i\geq0,\quad i=1,2,\dots,N
\end{aligned}
$$

<h2 id="1.3">线性不可分线性支持向量机</h2>

$$
\begin{aligned}
\min_{w,b,\xi}&\quad\frac{1}{2}||w||^2 +C\sum_{i=1}^N\xi_i\\
s.t.&\quad y_i(w^Tx_i+b)\geq 1-\xi_i,\quad i=1,2,\dots,N \\
&\quad\xi_i\geq0,\quad i=1, 2,\dots,N
\end{aligned}
$$

<h2 id="1.4">参考资料</h2>

1. [https://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html](https://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html)
2. [https://zhuanlan.zhihu.com/p/24638007](https://zhuanlan.zhihu.com/p/24638007)
3. [https://www.cnblogs.com/xxrxxr/p/7535587.html](https://www.cnblogs.com/xxrxxr/p/7535587.html)
3. [统计学习方法—李航](http://vdisk.weibo.com/s/vfFpMc1YgPOr)
4. [https://blog.csdn.net/luoshixian099/article/details/51227754](https://blog.csdn.net/luoshixian099/article/details/51227754)
5. [cs231n](https://www.bilibili.com/video/av17204303?from=search&seid=11474099995769581382)
6. [http://blog.pluskid.org/?p=685](http://blog.pluskid.org/?p=685)
